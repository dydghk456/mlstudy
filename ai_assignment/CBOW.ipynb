{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3267e7b9",
      "metadata": {
        "id": "3267e7b9"
      },
      "source": [
        "# AI Homework 5\n",
        "In Homework 5 We will train our own 'CBOW' Word2Vec embedding from WikiText2 dataset. (small dataset)\n",
        "- Change Runtime option above to GPU if you could. (max 12 hours for one user)\n",
        "- Save and submit the output of this notebook and model and vocab file you trained.\n",
        "- not allowed to have other python file or import pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOU should run this command if you will train the model in COLAB environment\n",
        "! pip install datasets transformers"
      ],
      "metadata": {
        "id": "fEUOIf5b55uz"
      },
      "id": "fEUOIf5b55uz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83999a8f",
      "metadata": {
        "scrolled": true,
        "id": "83999a8f"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import yaml\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "\n",
        "import json\n",
        "import numpy as np \n",
        "\n",
        "from functools import partial\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import WikiText2 # WikiText103\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1c48ed1",
      "metadata": {
        "id": "d1c48ed1"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch_seed_numb = 0\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.manual_seed(torch_seed_numb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "020510ad",
      "metadata": {
        "id": "020510ad"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you use Google Colab environment, mount you google drive here to save model and vocab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root_dir = '/content/drive/MyDrive/course_ai_hw5'"
      ],
      "metadata": {
        "id": "YYgqjYSdBvDW"
      },
      "id": "YYgqjYSdBvDW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f2f991d",
      "metadata": {
        "id": "7f2f991d"
      },
      "outputs": [],
      "source": [
        "# You could change parameters if you want.\n",
        "\n",
        "train_batch_size =  96\n",
        "val_batch_size = 96\n",
        "shuffle =  True\n",
        "\n",
        "optimizer =  'Adam'\n",
        "learning_rate =  0.025\n",
        "epochs =  5\n",
        "\n",
        "result_dir = 'weights/' \n",
        "\n",
        "# Parameters about CBOW model architecture and Vocab.\n",
        "CBOW_N_WORDS = 4\n",
        "\n",
        "MIN_WORD_FREQUENCY = 50\n",
        "MAX_SEQUENCE_LENGTH = 256\n",
        "\n",
        "EMBED_DIMENSION = 300\n",
        "EMBED_MAX_NORM = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_dir = os.path.join(root_dir, result_dir)\n",
        "if not os.path.exists(result_dir):\n",
        "    os.mkdir(result_dir)\n"
      ],
      "metadata": {
        "id": "3FgsAeWUDvzI"
      },
      "id": "3FgsAeWUDvzI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare dataset and vocab"
      ],
      "metadata": {
        "id": "xQacYYHCovzA"
      },
      "id": "xQacYYHCovzA"
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
        "train_dataset = datasets[\"train\"]\n",
        "val_dataset = datasets['validation']\n",
        "test_dataset = datasets['test']\n",
        "#train_dataset.map(tokenizing_word , batched= True, batch_size = 5000)\n"
      ],
      "metadata": {
        "id": "rCx3DsJr5RxX"
      },
      "id": "rCx3DsJr5RxX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's print one example\n",
        "train_dataset['text'][11]"
      ],
      "metadata": {
        "id": "j499GQWrxOma"
      },
      "id": "j499GQWrxOma",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, We should clean and lower sentences, tokenize sentences and change each word to index (one-hot-vector). Before going throught the whole process we should make vocabulary using train dataset in order to make each word to index."
      ],
      "metadata": {
        "id": "ulQoM_SixZBT"
      },
      "id": "ulQoM_SixZBT"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
        "\n",
        "# TO DO 1) make vocabulary \n",
        "# Hint) use function: build_vocab_from_iterator, use train_dataset set special tokens.. etc\n",
        "\n"
      ],
      "metadata": {
        "id": "u63stoTf7oRn"
      },
      "id": "u63stoTf7oRn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a collate function in order to make dataset into CBOW train format. The collate function should iterate over (sliding) batch data and make train/test dataset. And each component of data should be composed of CBOW_N_WORD words in left and right side as input and target output as word in center.  \n",
        "Make the collate function return CBOW dataset in tensor type.  \n",
        "- "
      ],
      "metadata": {
        "id": "i8z22T68aT-m"
      },
      "id": "i8z22T68aT-m"
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is a lambda function to tokenize sentence and change words to vocab indexes.\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))"
      ],
      "metadata": {
        "id": "CrDMIbXVHcbk"
      },
      "id": "CrDMIbXVHcbk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cbow](https://user-images.githubusercontent.com/74028313/204695601-51d44a38-4bd3-4a69-8891-2854aa57c034.png)"
      ],
      "metadata": {
        "id": "yl65bTOrEuSU"
      },
      "id": "yl65bTOrEuSU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "745eedd1",
      "metadata": {
        "id": "745eedd1"
      },
      "outputs": [],
      "source": [
        "def collate(batch, text_pipeline):\n",
        "\n",
        "    batch_input, batch_output = [], []\n",
        "    \n",
        "    # TO DO 2): make collate function\n",
        "\n",
        "    return batch_input, batch_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8c062f3",
      "metadata": {
        "id": "b8c062f3"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    train_dataset['text'],\n",
        "    batch_size=train_batch_size,\n",
        "    shuffle=shuffle,\n",
        "    collate_fn=partial(collate, text_pipeline=text_pipeline),\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset['text'],\n",
        "    batch_size=val_batch_size,\n",
        "    shuffle=shuffle,\n",
        "    collate_fn=partial(collate, text_pipeline=text_pipeline),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make CBOW Model\n",
        "![image](https://user-images.githubusercontent.com/74028313/204701161-cd9df4bf-78b8-4b4d-b8b7-ed4a3b5c3922.png)\n",
        "\n",
        "CBOW Models' main concept is to predict center-target word using context words. As you see in above simple architecture, input 2XCBOW_N_WORDS length words are projected to Projection layer. In order to convert each word to embedding, it needs look-up table and we will use torch's Embedding function to convert it. After combining embeddings of context, it use shallow linear neural network to predict target word and compare result with center word's index using cross-entropy loss. Finally, the embedding layer (lookup table) of the trained model itself serves as an embedding representing words."
      ],
      "metadata": {
        "id": "95_n2reEdl8-"
      },
      "id": "95_n2reEdl8-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5066a121",
      "metadata": {
        "id": "5066a121"
      },
      "outputs": [],
      "source": [
        "class CBOW_Model(nn.Module):\n",
        "    def __init__(self, vocab_size: int, EMBED_DIMENSION, EMBED_MAX_NORM):\n",
        "        super(CBOW_Model, self).__init__()\n",
        "        # TO DO 3-1): make CBOW model using nn.Embedding and nn.Linear function\n",
        "    \n",
        "\n",
        "    def forward(self, _inputs):\n",
        "        # TO DO 3-2): make forward function\n",
        "\n",
        "\n",
        "        return _outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e37edd6",
      "metadata": {
        "id": "3e37edd6"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Let's train our CBOW model, make _train_epoch and _validate_epoch function.  \n",
        "- model.train() and model.eval() change torch mode in some parts (Dropout, BatchNorm..  etc) of the model to behave differently during inference time. \n",
        "- train model with constant learning rate first, There is lr_scheduler option which changes learning rate according to epoch level. Try the option if you are interested in. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab.get_stoi())\n",
        "\n",
        "model = CBOW_Model(vocab_size=vocab_size, EMBED_DIMENSION = EMBED_DIMENSION, EMBED_MAX_NORM = EMBED_MAX_NORM)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "gN4OgGATqry3"
      },
      "id": "gN4OgGATqry3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cda8d29",
      "metadata": {
        "id": "0cda8d29"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Train_CBOW:\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        epochs,\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        loss_function,\n",
        "        optimizer,\n",
        "        device,\n",
        "        model_dir,\n",
        "        lr_scheduler = None\n",
        "    ):  \n",
        "        self.model = model\n",
        "        self.epochs = epochs\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.loss_function = loss_function\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_scheduler = lr_scheduler\n",
        "        self.device = device\n",
        "        self.model_dir = model_dir\n",
        "\n",
        "        self.loss = {\"train\": [], \"val\": []}\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.epochs):\n",
        "            self._train_epoch()\n",
        "            self._validate_epoch()\n",
        "            print(\n",
        "                \"Epoch: {}/{}, Train Loss={:.5f}, Val Loss={:.5f}\".format(\n",
        "                    epoch + 1,\n",
        "                    self.epochs,\n",
        "                    self.loss[\"train\"][-1],\n",
        "                    self.loss[\"val\"][-1],\n",
        "                )\n",
        "            )\n",
        "            if self.lr_scheduler is not None:\n",
        "                self.lr_scheduler.step()\n",
        "\n",
        "\n",
        "    def _train_epoch(self):\n",
        "        self.model.train() # set model as train \n",
        "        loss_list = []\n",
        "        # TO DO 4-1):\n",
        "\n",
        "\n",
        "        # end of TO DO \n",
        "        epoch_loss = np.mean(loss_list)\n",
        "        self.loss[\"train\"].append(epoch_loss)\n",
        "\n",
        "    def _validate_epoch(self):\n",
        "        self.model.eval()\n",
        "        loss_list = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # TO DO 4-2): \n",
        "\n",
        "            # end of TO DO \n",
        "        epoch_loss = np.mean(loss_list)\n",
        "        self.loss[\"val\"].append(epoch_loss)\n",
        "        \n",
        "\n",
        "    def save_model(self):\n",
        "        model_path = os.path.join(self.model_dir, \"model.pt\")\n",
        "        torch.save(self.model, model_path)\n",
        "\n",
        "    def save_loss(self):\n",
        "        loss_path = os.path.join(self.model_dir, \"loss.json\")\n",
        "        with open(loss_path, \"w\") as fp:\n",
        "            json.dump(self.loss, fp)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Option: you could add and change lr_sceduler \n",
        "scheduler = LambdaLR(optimizer, lr_lambda = lambda epoch: 0.95 ** epoch)"
      ],
      "metadata": {
        "id": "fcx2Ms537jwR"
      },
      "id": "fcx2Ms537jwR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36edc874",
      "metadata": {
        "id": "36edc874"
      },
      "outputs": [],
      "source": [
        "trainer = Train_CBOW(\n",
        "    model=model,\n",
        "    epochs=epochs,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    loss_function=loss_function,\n",
        "    optimizer=optimizer,\n",
        "    lr_scheduler=None,\n",
        "    device=device,\n",
        "    model_dir=result_dir,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"Training finished.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "840866c3",
      "metadata": {
        "id": "840866c3"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "trainer.save_model()\n",
        "trainer.save_loss()\n",
        "\n",
        "vocab_path = os.path.join(result_dir, \"vocab.pt\")\n",
        "torch.save(vocab, vocab_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9373acbe",
      "metadata": {
        "id": "9373acbe"
      },
      "source": [
        "### Result\n",
        "Let's inference trained word embedding and visualize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d19035",
      "metadata": {
        "id": "a6d19035"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "sys.path.append(\"../\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_dir"
      ],
      "metadata": {
        "id": "fNszs2GvMbbz"
      },
      "id": "fNszs2GvMbbz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba4c1ad2",
      "metadata": {
        "id": "ba4c1ad2"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# reload saved model and vocab\n",
        "model = torch.load(os.path.join(result_dir,\"model.pt\"), map_location=device)\n",
        "vocab = torch.load(os.path.join(result_dir,\"vocab.pt\"))\n",
        "\n",
        "# embedding is model's first layer\n",
        "embeddings = list(model.parameters())[0]\n",
        "embeddings = embeddings.cpu().detach().numpy()\n",
        "\n",
        "# normalization\n",
        "norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
        "norms = np.reshape(norms, (len(norms), 1))\n",
        "embeddings_norm = embeddings / norms\n",
        "embeddings_norm.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-1) Make TSNE graph of trained embedding and color numeric values "
      ],
      "metadata": {
        "id": "arTAGVNbuxvP"
      },
      "id": "arTAGVNbuxvP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4e2d8a7",
      "metadata": {
        "id": "e4e2d8a7"
      },
      "outputs": [],
      "source": [
        "embeddings_df = pd.DataFrame(embeddings)\n",
        "fig = go.Figure()\n",
        "# TO DO 5-1) : make 2-d TSNE graph of all vocabs and color numeric values only\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce44537c",
      "metadata": {
        "id": "ce44537c"
      },
      "source": [
        "### 5-2) find top N similar words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fa05e68",
      "metadata": {
        "id": "4fa05e68"
      },
      "outputs": [],
      "source": [
        "def get_top_similar(word: str, vocab, embeddings_norm, topN: int = 10):\n",
        "    # TO DO 5-2) : make function returning top n similiar words and similarity scores\n",
        "    topN_dict = {}\n",
        "    \n",
        "    return topN_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74887441",
      "metadata": {
        "id": "74887441"
      },
      "outputs": [],
      "source": [
        "for word, sim in get_top_similar(\"english\", vocab, embeddings_norm).items():\n",
        "    print(\"{}: {:.3f}\".format(word, sim))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result Report\n",
        "\n",
        "Save the colab result and submit it with your trained model and vocab file. Check one more time your submitted notebook file has result. \n",
        "\n",
        "You can change the CBOW model parameters Training parameters and details if you want."
      ],
      "metadata": {
        "id": "h9WjlWvx0638"
      },
      "id": "h9WjlWvx0638"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aecdbb1d",
      "metadata": {
        "id": "aecdbb1d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e231719",
      "metadata": {
        "id": "0e231719"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf8b791",
      "metadata": {
        "id": "caf8b791"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0100516",
      "metadata": {
        "id": "a0100516"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.13 ('frankcontact3.8')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "80dc11610feb6beaadd3581ff37d967363a9f0082fe81e3d7783a2cab15c2f20"
      }
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}