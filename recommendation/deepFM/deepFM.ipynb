{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4310ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85dd1aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_COLS = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "             'marital-status', 'occupation', 'relationship', 'race',\n",
    "             'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'country']\n",
    "CONT_COLS = ['age', 'fnlwgt', 'education-num',\n",
    "               'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "CAT_COLS = list(set(ALL_COLS).difference(CONT_COLS))\n",
    "\n",
    "NUM_BINS = 10\n",
    "BATCH_SIZE = 256\n",
    "EMBEDDING_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e41d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def get_modified_data(X, all_cols, cont_cols, cat_cols):\n",
    "    \n",
    "    X_cat = X[CAT_COLS]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_ret = pd.DataFrame(scaler.fit_transform(X[CONT_COLS]), columns=CONT_COLS)\n",
    "    cols_idx = list(range(X_ret.shape[1]))\n",
    "    cat_start_idx = X_ret.shape[1]\n",
    "    \n",
    "    for idx, col in enumerate(X_cat):\n",
    "        cat_X = pd.get_dummies(X_cat[col], prefix=col, prefix_sep='-')\n",
    "        cols_idx.extend(repeat(idx + cat_start_idx, cat_X.shape[1]))\n",
    "        X_ret = pd.concat([X_ret, cat_X], axis=1)\n",
    "        \n",
    "    print('X shape: {}'.format(X_ret.shape))\n",
    "\n",
    "    return cols_idx, X_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38010874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_feature, num_field, embedding_size, field_index):\n",
    "        super(FM, self).__init__()\n",
    "        self.embedding_size = embedding_size    \n",
    "        self.num_feature = num_feature         \n",
    "        self.num_field = num_field              \n",
    "        self.field_index = field_index          \n",
    "\n",
    "        self.w = tf.Variable(tf.random.normal(shape=[num_feature], mean=0.0, stddev=1.0), name='w')\n",
    "        self.V = tf.Variable(tf.random.normal(shape=(num_field, embedding_size), mean=0.0, stddev=0.01), name='V')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_batch = tf.reshape(inputs, [-1, self.num_feature, 1])\n",
    "        embedding = tf.nn.embedding_lookup(params=self.V, ids=self.field_index)\n",
    "        #(batch size, feature num, embedding size)\n",
    "        embedded_inputs = tf.math.multiply(x_batch, embedding)\n",
    "\n",
    "        #(batch size, )\n",
    "        ord1_terms = tf.reduce_sum(tf.math.multiply(self.w, inputs), axis=1, keepdims=False)\n",
    "        ord2_terms = 0.5 * tf.subtract(tf.square(tf.reduce_sum(embedded_inputs, [1, 2])), tf.reduce_sum(tf.square(embedded_inputs), [1, 2]))\n",
    "        \n",
    "        \n",
    "        ord1_terms = tf.reshape(ord1_terms, [-1, 1])\n",
    "        ord2_terms = tf.reshape(ord2_terms, [-1, 1])\n",
    "\n",
    "        y_fm = tf.concat([ord1_terms, ord2_terms], 1)\n",
    "\n",
    "        return y_fm, embedded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d1f8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "class DeepFM(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_feature, num_field, embedding_size, field_index):\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.embedding_size = embedding_size    # k: 임베딩 벡터의 차원(크기)\n",
    "        self.num_feature = num_feature          # f: 원래 feature 개수\n",
    "        self.num_field = num_field              # m: grouped field 개수\n",
    "        self.field_index = field_index          # 인코딩된 X의 칼럼들이 본래 어디 소속이었는지\n",
    "\n",
    "        self.fm = FM(num_feature, num_field, embedding_size, field_index)\n",
    "\n",
    "        self.layer1 = tf.keras.layers.Dense(units=64, activation='relu')\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate=0.2)\n",
    "        self.layer2 = tf.keras.layers.Dense(units=16, activation='relu')\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate=0.2)\n",
    "        self.layer3 = tf.keras.layers.Dense(units=2, activation='relu')\n",
    "\n",
    "        self.predict = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        y_fm, embedded_inputs = self.fm(inputs)\n",
    "\n",
    "        embedded_inputs = tf.reshape(embedded_inputs, [-1, self.num_feature*self.embedding_size])\n",
    "\n",
    "        #deep component\n",
    "        y_deep = self.layer1(embedded_inputs)\n",
    "        y_deep = self.dropout1(y_deep)\n",
    "        y_deep = self.layer2(y_deep)\n",
    "        y_deep = self.dropout2(y_deep)\n",
    "        y_deep = self.layer3(y_deep)\n",
    "\n",
    "        # Concatenation\n",
    "        y_pred = self.predict(tf.concat([y_fm, y_deep], axis=1))\n",
    "        y_pred = tf.reshape(y_pred, [-1, ])\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b2ecd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    data = pd.read_csv('data/adult.data', header=None)\n",
    "    X = data.loc[:, 0:13]\n",
    "    Y = data.loc[:, 14].map({' <=50K': 0, ' >50K': 1})\n",
    "\n",
    "    X.columns = ALL_COLS\n",
    "    field_index, X_embedded = get_modified_data(X, ALL_COLS, CONT_COLS, CAT_COLS)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_embedded, Y, test_size=0.2, stratify=Y)\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((tf.cast(X_train.values, tf.float32), tf.cast(Y_train, tf.float32))).shuffle(30000).batch(BATCH_SIZE)\n",
    "\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((tf.cast(X_test.values, tf.float32), tf.cast(Y_test, tf.float32))).shuffle(10000).batch(BATCH_SIZE)\n",
    "\n",
    "    return train_ds, test_ds, field_index, X.shape[1]\n",
    "\n",
    "\n",
    "def train_on_batch(model, optimizer, acc, auc, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(inputs)\n",
    "        loss = tf.keras.losses.binary_crossentropy(from_logits=False, y_true=targets, y_pred=y_pred)\n",
    "\n",
    "    grads = tape.gradient(target=loss, sources=model.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    acc.update_state(targets, y_pred)\n",
    "    auc.update_state(targets, y_pred)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(epochs):\n",
    "    train_ds, test_ds, field_index, num_field = get_data()\n",
    "\n",
    "    model = DeepFM(embedding_size=EMBEDDING_SIZE, num_feature=len(field_index), num_field=num_field, field_index=field_index)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "    print(\"Start Training\")\n",
    "    start = perf_counter()\n",
    "    for i in range(epochs):\n",
    "        acc = tf.keras.metrics.BinaryAccuracy(threshold=0.5)\n",
    "        auc = tf.keras.metrics.AUC()\n",
    "        loss_history = []\n",
    "\n",
    "        for x, y in train_ds:\n",
    "            loss = train_on_batch(model, optimizer, acc, auc, x, y)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "        print(\"Epoch {:03d}: Loss: {:.4f}, Acc: {:.4f}, AUC: {:.4f}\".format(i, loss_history[-1], acc.result().numpy(), auc.result().numpy()))\n",
    "\n",
    "    test_acc = tf.keras.metrics.BinaryAccuracy(threshold=0.5)\n",
    "    test_auc = tf.keras.metrics.AUC()\n",
    "    for x, y in test_ds:\n",
    "        y_pred = model(x)\n",
    "        test_acc.update_state(y, y_pred)\n",
    "        test_auc.update_state(y, y_pred)\n",
    "\n",
    "    print(\"test ACC: {:.4f}, AUC: {:.4f}\".format(test_acc.result().numpy(), test_auc.result().numpy()))\n",
    "    print(\"time: {:.3f}\".format(perf_counter() - start))\n",
    "    #model.save_weights('weights/weights-epoch({})-batch({})-embedding({}).h5'.format(epochs, BATCH_SIZE, EMBEDDING_SIZE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35b23144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (32561, 108)\n",
      "Start Training\n",
      "Epoch 000: Loss: 0.8519, Acc: 0.6177, AUC: 0.2836\n",
      "Epoch 001: Loss: 0.8216, Acc: 0.6349, AUC: 0.3102\n",
      "Epoch 002: Loss: 0.8126, Acc: 0.6488, AUC: 0.3415\n",
      "Epoch 003: Loss: 0.7942, Acc: 0.6593, AUC: 0.3758\n",
      "Epoch 004: Loss: 0.8416, Acc: 0.6693, AUC: 0.4125\n",
      "Epoch 005: Loss: 0.7107, Acc: 0.6806, AUC: 0.4516\n",
      "Epoch 006: Loss: 0.6836, Acc: 0.6904, AUC: 0.4935\n",
      "Epoch 007: Loss: 0.6307, Acc: 0.6990, AUC: 0.5364\n",
      "Epoch 008: Loss: 0.6119, Acc: 0.7062, AUC: 0.5747\n",
      "Epoch 009: Loss: 0.5196, Acc: 0.7134, AUC: 0.6077\n",
      "Epoch 010: Loss: 0.6139, Acc: 0.7187, AUC: 0.6369\n",
      "Epoch 011: Loss: 0.5175, Acc: 0.7238, AUC: 0.6629\n",
      "Epoch 012: Loss: 0.5328, Acc: 0.7279, AUC: 0.6865\n",
      "Epoch 013: Loss: 0.5484, Acc: 0.7324, AUC: 0.7081\n",
      "Epoch 014: Loss: 0.4864, Acc: 0.7381, AUC: 0.7262\n",
      "Epoch 015: Loss: 0.5446, Acc: 0.7431, AUC: 0.7415\n",
      "Epoch 016: Loss: 0.5411, Acc: 0.7503, AUC: 0.7545\n",
      "Epoch 017: Loss: 0.5358, Acc: 0.7578, AUC: 0.7655\n",
      "Epoch 018: Loss: 0.5124, Acc: 0.7644, AUC: 0.7752\n",
      "Epoch 019: Loss: 0.4249, Acc: 0.7706, AUC: 0.7833\n",
      "Epoch 020: Loss: 0.4640, Acc: 0.7750, AUC: 0.7909\n",
      "Epoch 021: Loss: 0.4506, Acc: 0.7789, AUC: 0.7974\n",
      "Epoch 022: Loss: 0.4233, Acc: 0.7814, AUC: 0.8033\n",
      "Epoch 023: Loss: 0.4880, Acc: 0.7841, AUC: 0.8091\n",
      "Epoch 024: Loss: 0.4758, Acc: 0.7876, AUC: 0.8141\n",
      "Epoch 025: Loss: 0.4799, Acc: 0.7912, AUC: 0.8186\n",
      "Epoch 026: Loss: 0.4378, Acc: 0.7939, AUC: 0.8230\n",
      "Epoch 027: Loss: 0.3574, Acc: 0.7960, AUC: 0.8267\n",
      "Epoch 028: Loss: 0.4714, Acc: 0.7989, AUC: 0.8302\n",
      "Epoch 029: Loss: 0.4153, Acc: 0.8013, AUC: 0.8334\n",
      "Epoch 030: Loss: 0.3616, Acc: 0.8024, AUC: 0.8364\n",
      "Epoch 031: Loss: 0.4261, Acc: 0.8053, AUC: 0.8390\n",
      "Epoch 032: Loss: 0.3393, Acc: 0.8066, AUC: 0.8415\n",
      "Epoch 033: Loss: 0.5162, Acc: 0.8074, AUC: 0.8438\n",
      "Epoch 034: Loss: 0.3839, Acc: 0.8089, AUC: 0.8459\n",
      "Epoch 035: Loss: 0.3923, Acc: 0.8110, AUC: 0.8479\n",
      "Epoch 036: Loss: 0.3951, Acc: 0.8122, AUC: 0.8499\n",
      "Epoch 037: Loss: 0.3795, Acc: 0.8138, AUC: 0.8516\n",
      "Epoch 038: Loss: 0.4954, Acc: 0.8149, AUC: 0.8535\n",
      "Epoch 039: Loss: 0.3916, Acc: 0.8153, AUC: 0.8554\n",
      "Epoch 040: Loss: 0.4840, Acc: 0.8181, AUC: 0.8567\n",
      "Epoch 041: Loss: 0.4411, Acc: 0.8191, AUC: 0.8585\n",
      "Epoch 042: Loss: 0.3604, Acc: 0.8196, AUC: 0.8600\n",
      "Epoch 043: Loss: 0.3581, Acc: 0.8213, AUC: 0.8613\n",
      "Epoch 044: Loss: 0.4185, Acc: 0.8215, AUC: 0.8630\n",
      "Epoch 045: Loss: 0.4521, Acc: 0.8220, AUC: 0.8644\n",
      "Epoch 046: Loss: 0.3866, Acc: 0.8233, AUC: 0.8659\n",
      "Epoch 047: Loss: 0.4028, Acc: 0.8241, AUC: 0.8672\n",
      "Epoch 048: Loss: 0.3460, Acc: 0.8252, AUC: 0.8685\n",
      "Epoch 049: Loss: 0.3626, Acc: 0.8269, AUC: 0.8702\n",
      "Epoch 050: Loss: 0.4155, Acc: 0.8272, AUC: 0.8714\n",
      "Epoch 051: Loss: 0.3002, Acc: 0.8279, AUC: 0.8726\n",
      "Epoch 052: Loss: 0.3400, Acc: 0.8298, AUC: 0.8742\n",
      "Epoch 053: Loss: 0.4454, Acc: 0.8293, AUC: 0.8754\n",
      "Epoch 054: Loss: 0.3655, Acc: 0.8316, AUC: 0.8767\n",
      "Epoch 055: Loss: 0.4554, Acc: 0.8308, AUC: 0.8777\n",
      "Epoch 056: Loss: 0.4130, Acc: 0.8323, AUC: 0.8789\n",
      "Epoch 057: Loss: 0.3033, Acc: 0.8329, AUC: 0.8801\n",
      "Epoch 058: Loss: 0.3269, Acc: 0.8343, AUC: 0.8811\n",
      "Epoch 059: Loss: 0.3956, Acc: 0.8350, AUC: 0.8823\n",
      "Epoch 060: Loss: 0.3071, Acc: 0.8349, AUC: 0.8831\n",
      "Epoch 061: Loss: 0.4076, Acc: 0.8358, AUC: 0.8841\n",
      "Epoch 062: Loss: 0.3125, Acc: 0.8356, AUC: 0.8850\n",
      "Epoch 063: Loss: 0.3444, Acc: 0.8368, AUC: 0.8859\n",
      "Epoch 064: Loss: 0.3361, Acc: 0.8372, AUC: 0.8866\n",
      "Epoch 065: Loss: 0.4344, Acc: 0.8368, AUC: 0.8873\n",
      "Epoch 066: Loss: 0.3612, Acc: 0.8378, AUC: 0.8882\n",
      "Epoch 067: Loss: 0.4210, Acc: 0.8384, AUC: 0.8889\n",
      "Epoch 068: Loss: 0.3844, Acc: 0.8382, AUC: 0.8895\n",
      "Epoch 069: Loss: 0.2870, Acc: 0.8394, AUC: 0.8902\n",
      "Epoch 070: Loss: 0.3916, Acc: 0.8390, AUC: 0.8909\n",
      "Epoch 071: Loss: 0.3516, Acc: 0.8392, AUC: 0.8914\n",
      "Epoch 072: Loss: 0.3549, Acc: 0.8402, AUC: 0.8918\n",
      "Epoch 073: Loss: 0.3959, Acc: 0.8399, AUC: 0.8922\n",
      "Epoch 074: Loss: 0.3786, Acc: 0.8400, AUC: 0.8928\n",
      "Epoch 075: Loss: 0.3078, Acc: 0.8400, AUC: 0.8932\n",
      "Epoch 076: Loss: 0.3239, Acc: 0.8402, AUC: 0.8933\n",
      "Epoch 077: Loss: 0.3061, Acc: 0.8413, AUC: 0.8938\n",
      "Epoch 078: Loss: 0.4228, Acc: 0.8408, AUC: 0.8942\n",
      "Epoch 079: Loss: 0.3223, Acc: 0.8421, AUC: 0.8946\n",
      "Epoch 080: Loss: 0.2977, Acc: 0.8414, AUC: 0.8948\n",
      "Epoch 081: Loss: 0.2348, Acc: 0.8423, AUC: 0.8952\n",
      "Epoch 082: Loss: 0.3096, Acc: 0.8421, AUC: 0.8953\n",
      "Epoch 083: Loss: 0.3202, Acc: 0.8426, AUC: 0.8955\n",
      "Epoch 084: Loss: 0.2883, Acc: 0.8421, AUC: 0.8958\n",
      "Epoch 085: Loss: 0.3323, Acc: 0.8416, AUC: 0.8959\n",
      "Epoch 086: Loss: 0.3437, Acc: 0.8421, AUC: 0.8963\n",
      "Epoch 087: Loss: 0.3385, Acc: 0.8422, AUC: 0.8964\n",
      "Epoch 088: Loss: 0.3043, Acc: 0.8423, AUC: 0.8967\n",
      "Epoch 089: Loss: 0.3584, Acc: 0.8427, AUC: 0.8969\n",
      "Epoch 090: Loss: 0.3474, Acc: 0.8432, AUC: 0.8971\n",
      "Epoch 091: Loss: 0.3480, Acc: 0.8440, AUC: 0.8971\n",
      "Epoch 092: Loss: 0.3055, Acc: 0.8441, AUC: 0.8975\n",
      "Epoch 093: Loss: 0.3241, Acc: 0.8438, AUC: 0.8976\n",
      "Epoch 094: Loss: 0.3581, Acc: 0.8428, AUC: 0.8977\n",
      "Epoch 095: Loss: 0.3079, Acc: 0.8435, AUC: 0.8979\n",
      "Epoch 096: Loss: 0.4057, Acc: 0.8433, AUC: 0.8982\n",
      "Epoch 097: Loss: 0.3446, Acc: 0.8432, AUC: 0.8982\n",
      "Epoch 098: Loss: 0.3997, Acc: 0.8435, AUC: 0.8982\n",
      "Epoch 099: Loss: 0.3351, Acc: 0.8443, AUC: 0.8985\n",
      "test ACC: 0.8448, AUC: 0.8972\n",
      "time: 453.526\n"
     ]
    }
   ],
   "source": [
    "train(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c909e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
