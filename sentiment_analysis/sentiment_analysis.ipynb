{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9c62c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bce15d80",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [94]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, file), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m infile:\n\u001b[0;32m     11\u001b[0m                 txt \u001b[38;5;241m=\u001b[39m infile\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m---> 12\u001b[0m             df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:8965\u001b[0m, in \u001b[0;36mDataFrame.append\u001b[1;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[0;32m   8962\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   8963\u001b[0m     to_concat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m, other]\n\u001b[0;32m   8964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 8965\u001b[0m     \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   8966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8969\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8970\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   8971\u001b[0m )\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:307\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03mConcatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03malong the other axes.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03mValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    295\u001b[0m     objs,\n\u001b[0;32m    296\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    305\u001b[0m )\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:532\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    530\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 532\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy:\n\u001b[0;32m    536\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py:216\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    210\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m#  than concat_compat\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train 폴더와 test 폴더에 있는 positive, negative comments들과 그 라벨을 df에 append한다. (positive = 1, negative = 0)\n",
    "basepath = 'aclImdb'\n",
    "\n",
    "labels = {'pos':1, 'neg':0}\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos','neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "\n",
    "df.columns = ['review','sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1471d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df엔 양성과 음성이 순서대로 나열되어 있으므로 섞어줌\n",
    "df = df.iloc[np.random.permutation(df.index)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "576a9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv파일로 df를 저장\n",
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75e480ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>','',str(text)) # HTML 태그들을 삭제\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|p)', text) #이모티콘들은 감정분석에 도움이 되기 때문에 삭제하지 않음\n",
    "    text = (re.sub('[W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')) #문자나 숫자가 아닌 것을 모두 삭제 후, 이모티콘을 붙임\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv) #헤더는 넘김\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2fe0e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0cbc5e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65cb0aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "897246e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', \"doesn't\", 'kill', 'you', 'make', 'you', 'stronger']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#단어를 변하지 않는 기본 형태인 어간으로 바꾸는 어간 추출(stemming), makes -> make\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "tokenizer_porter('what doesn\\'t kill you makes you stronger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "08f993be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 제거, 불용어는 아주 흔하게 등장하는 단어로 문서의 종류를 구별하는데 별 도움이 안되는 단어들임 (is, and, has ..)\n",
    "#tf idf vectorizer 에선, 특정한 단어가 많은 문서에서 등장할수록 그 단어의 가중치가 이미 낮아져 있음(term frequency inverse document frequency)\n",
    "#따라서 불용어 제거는 tf-idf보다 기본 단어 빈도나 정규화된 단어 빈도를 사용할 때 더 도움됨.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "37112727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c97fb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_text = train_test_split(df['review'],df['sentiment'], test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3de18902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8c4d43a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSeachCV를 통해 최적의 매개변수를 구함\n",
    "#토크나이저로는 그냥 스플릿하는 토크나이저와 어간을 추출하는 토크나이저르르 비교, 분류기의 규제와 규제 강도도 조절\n",
    "#또한, idf값도 사용하는 것과 사용하지 않는 것을 비교\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "param_grid = [{'vect__ngram_range':[1,1], 'vect__stop_words':[stop, None], 'vect__tokenizer':[tokenizer, tokenizer_porter],\n",
    "              'clf__penalty':['l1','l2'], 'clf__C':[1.0,10.0,100.0]},\n",
    "             {'vect__ngram_range':[(1,1)], 'vect__stop_words':[stop,None], \"vect__tokenizer\":[tokenizer, tokenizer_porter],\n",
    "             'vect__use_idf':[False], 'vect__norm':[None], 'clf__penalty':['l1','l2'], 'clf__C':[1.0,10.0,100.0]}\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e40236",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(solver='liblinear', random_state=0))])\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "gs_lr_tfidf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c4bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gs_lr_tfidf.best_estimator\n",
    "print(\"CV 정확도 %s 테스트 정확도 %s\" %(gs_lr_tfidf.best_score_, clf.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e670b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#한글을 다룰 땐 표제어를 추출하는 것이 더 좋은 방법임. 이러한 작업을 형태소 분석이라고 부름\n",
    "#이러한 형태소 분석 작업 konlpy패키지를 통해 할 수 있음.( eg) Okt 클래스(open-korean-text)), 0kt.morphs 메소드를 사용하여 토큰화.\n",
    "#soynlp도 3개의 형태소 분석기를 제공 LTokenizer는 띄어쓰기를 잘 한 샘플에 알맞음. .tokenize 메소드로 토큰화\n",
    "#그 외에는 MaxScroeTokenizer, RegexTokenizer가 잘 맞음\n",
    "#soynlp는 WordExtractor객체를 통해 통계 데이터를 생성하고, 이를 LTokenizer에 매개변수로 줄 수 있음.\n",
    "#주지 않을 시 말뭉치의 통계를 기반으로 진행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f9bac772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"Almost in the same league as Yonfan\\'s rather atrocious Color Blossoms, Spider Lillies drives the point home that you can make cutting edge cinema without the edge, or much in the way of cutting. It\\'s a Taiwanese film, which in this day and age is becoming a novelty at an alarming pace, but more than that tidbit, we can find very little in the way of the noteworthy here.<br /><br />You should know that ostensibly Spider Lillies is also a lesbian-themed story, but in every aspect this is nothing but a plastic ploy to lure in the easily seduced and gullible. In several ways we have here a repeat of fellow recent Taiwan release Eternal Summer. Then it was gay men getting the shortchange treatment, now we have the same thing with women. Zero Chou presents, for your non-existent edification, a tale likely to titillate at most a fifteen year old. They managed some of the art house stance, but in the end this results in a most inane, simply uninteresting foray.<br /><br />The Hong Kong angle comes in the form of Isabella Leung (Bug Me Not, Isabella, Diary), here sporting her most butch look yet. Although somewhat likable in her previous jobs, Isabella in Spider Lillies is listless and lacking in most departments. Either her heart wasn\\'t into it or the whole lesbian drama pitch didn\\'t quite appeal to her sensibilities.<br /><br />She does a Taipei tattoo artist who\\'s shy, reclusive and in charge of a mentally challenged younger brother, played by John Shen, who thankfully grants the movie its only thespian-related redeeming feature. Isabella\\'s character, oddly named Takeko but supposedly hailing from Hong Kong, soon hooks up with disaffected youth Jade (Rainie Yang from fondly-recalled Meteor Garden). The latter lives with her grandmother and has a whole list of grievances due to being left behind by her parents and life in general. Sure, the grandmother component works well and is touching, but otherwise Jade as a protagonist is just as unmoving as her counterpart Takeko.<br /><br />The two women share a past and lots of inadequately covered angst, with Jade working as a webcam girl while Takeko keeps her father\\'s legacy alive with a unique tattoo of a spider lilly emblazoned on her arm. Jade also wants to acquire this very design, which leads to Takeko exploring internal feelings of the issue via flashbacks and rather minimal discourse with the spunky Jade.<br /><br />Well, if there\\'s little discourse to write the homebase about, is at least the intercourse memorable? In a word, no. They kiss and feign doing the nasty close to the end, but just as Eternal Summer reminded us not long ago, there\\'s a gulf measured in lightyears between showing sexual content and making ticket buyers think they\\'re about to see sexual content.<br /><br />This cynical expectation-building seals Spider Lillies\\' fate. With a weak story, ho-hum acting and an overall dearth of relics to take away from the theater with you, this one kind of makes Color Blossoms look good, come to think of it. At least there we got a bit of Teresa Cheung\\'s mammaries. No, Spider Lillies is no AV masterpiece and should be stricken from the playlist of even the most mundane and timid GLB movie festival.<br /><br />Amazingly for a pseudo-indie release, not even the soundtrack and cinematography produce moments of inspiration. That\\'s just as well, since it makes passing on Spider Lillies much easier. Believe you us, avoid it and you won\\'t be missing out on anything good.<br /><br />Rating: * *\"',\n",
       " 0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gridsearch는 오래 걸리기 때문에 외부 메모리 학습을 사용. 이 방법은 데이터셋을 작은 배치로 나누어 분류기를 점점 학습시킴\n",
    "#이를 위해 문서에서 샘플을 하나씩 읽어오는 streamdocs함수 사용. \n",
    "#외부 메모리 학습에 countvectorizer 와 tfidfvectorizer는 사용할 수 없음. 각각 전체 어휘 사전과 역문서 빈도를 계산해야 함.\n",
    "#이는 hashingvectorizer를 사용하여 해결 가능. \n",
    "next(stream_docs(path = 'movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "099df004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e06cae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error='ignore', n_features=2**21, preprocessor=None, tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='log', random_state=1, max_iter=1)\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0e4a439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([0,1])\n",
    "for _ in range(45):\n",
    "    x_train, y_train = get_minibatch(doc_stream, size=1000) #1000개의 샘플을 읽어옴\n",
    "    if not x_train:\n",
    "        break\n",
    "    x_train = vect.transform(x_train) # 각 샘플을 hashingvectorizer로 벡터화\n",
    "    clf.partial_fit(x_train,y_train, classes = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6a02cae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test precision: 0.825\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "x_test = vect.transform(x_test)\n",
    "print(\"test precision: %.3f\" %clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c531e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(loss='log', max_iter=1, random_state=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.partial_fit(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06118674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#토픽 모델링이란 레이블이 없는 텍스트 문서에 토픽을 할당하는 비지도 학습의 클러스터링과 비슷하다고 할 수 있음\n",
    "#토픽 모델링 기법으로는 잠재 디리클레 할당(latent dirichlet allocation)이 있음. 이는 bow를 입력으로 받고, \n",
    "# 이를 문서-토픽 행렬, 단어-토픽 행렬로 분해함, 토픽 개수는 하이퍼파라미터로 수동으로 지정해주어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f5e2d5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df['review'].fillna(value='',inplace=True)\n",
    "count = CountVectorizer(stop_words='english', max_df=.1, max_features=5000)\n",
    "x = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a2bba663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=123, learning_method='batch')\n",
    "x_topics = lda.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "319355c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape #lda의 components에는 각 토픽에 대한 특성의 중요도가 오름차순으로 담겨있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "20d1edfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 1:horror effects gore dead killer\n",
      "topic 2:worst guy minutes money stupid\n",
      "topic 3:war american dvd oscar play\n",
      "topic 4:action game music original style\n",
      "topic 5:series comedy episode kids tv\n",
      "topic 6:role wife performance woman plays\n",
      "topic 7:script audience poor production worst\n",
      "topic 8:music black city western white\n",
      "topic 9:book version role original musical\n",
      "topic 10:family human beautiful father true\n"
     ]
    }
   ],
   "source": [
    "#각 토픽별로 가장 중요한 특성 5개 출력, 토픽 1의 단어들을 보면 호러 영화와 관련이 있음을 알 수 있음\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"topic %d:\" % (topic_idx+1), end='')\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "16cae46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "horror movie #1: \n",
      "The Salena Incident is set in Arizona where six death row inmates are being transfered from the state prison for reasons never explained, while driving along the heavily armed prison bus gets a flat & the driver is forced to pull off the road. Then two blonde birds turn up & after seducing the incom ...\n",
      "\n",
      "horror movie #2: \n",
      "***SPOILERS*** ***SPOILERS*** Some bunch of Afrikkaner-Hillbilly types are out in the desert looking for Diamonds when they find a hard mound in the middle of a sandy desert area. Spoilers: The dumbest one starts hitting the mound with a pick, and cracks it open. Then he looks into the hole and stic ...\n",
      "\n",
      "horror movie #3: \n",
      "A group of friends discover gold deep inside an old mine. But by taking the gold and thinking they've hit it big, they awaken a long dead miner who's Hell Bent on protecting his treasure. \"Miner's Massacre\" is a chintzy b-horror movie in the extreme. You've got all your familiar clichés, your group  ...\n",
      "\n",
      "horror movie #4: \n",
      "A killer, cannibal rapist is killed by a crazed cop on the scene of his latest murder. At his grave a cult have gathered with plans to resurrect him by peeing onto the grave. This of course works and he awakes ripping the guys penis off and he is back into his old killing ways with an all new zombie ...\n",
      "\n",
      "horror movie #5: \n",
      "The first 2/3 of this film wasn't that dissimilar to the American mummy films of the 30s and 40s. Two lovers in ancient Mexico dared to defy the law and were doomed to die. One became an Aztec mummy whose job it was to guard the sacred treasure and his lady love. And the lady was reincarnated in the ...\n"
     ]
    }
   ],
   "source": [
    "#horror영화의 리뷰일 확률이 가장 높은 리뷰 5개를 출력\n",
    "horror = x_topics[:,0].argsort()[::-1]\n",
    "for idx, movie_idx in enumerate(horror[:5]):\n",
    "    print(\"\\nhorror movie #%d: \"%(idx+1))\n",
    "    print(df['review'][movie_idx][:300],'...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b4062f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "dest = os.path.join('movieclassifier','pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "    \n",
    "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)\n",
    "pickle.dump(clf, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe55a320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
