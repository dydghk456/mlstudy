{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ec3508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 32)                4256      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,289\n",
      "Trainable params: 4,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "\n",
    "class SingleLayer:\n",
    "\n",
    "    def __init__(self, learning_rate=0.1, l1=0, l2=0):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.w_history = []\n",
    "        self.lr = learning_rate\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "    def forpass(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "\n",
    "    def backpropa(self, x, err):\n",
    "        m = len(x)\n",
    "        w_grad = np.dot(x.T, err)/m\n",
    "        b_grad = np.sum(err)/m\n",
    "        return w_grad, b_grad\n",
    "\n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        y = y.reshape(-1,1)\n",
    "        y_val = y_val.reshape(-1,1)\n",
    "        m = len(x)\n",
    "        self.w = np.ones((x.shape[1],1))\n",
    "        self.b = 0\n",
    "        self.w_history.append(self.w.copy())\n",
    "        for i in range(epochs):\n",
    "            z = self.forpass(x)\n",
    "            a = self.activation(z)\n",
    "            err = -(y-a)\n",
    "            w_grad, b_grad = self.backpropa(x, err)\n",
    "            w_grad += (self.l1*np.sign(self.w) + self.l2*self.w)/m\n",
    "            self.w -= self.lr*w_grad\n",
    "            self.b -= b_grad\n",
    "            self.w_history.append(self.w.copy())\n",
    "            a = np.clip(a, 1e-10, 1-1e-10)\n",
    "            loss = np.sum(-(y*np.log(a) + (1-y)*np.log(1-a)))\n",
    "            self.losses.append((loss+self.reg_loss())/m)\n",
    "            self.update_val_loss(x_val,y_val)\n",
    "\n",
    "    def activation(self, z):\n",
    "        z = np.clip(z, -100, None)\n",
    "        a = 1/ (1+ np.exp(-z))\n",
    "        return a\n",
    "\n",
    "    def predict(self, x):\n",
    "        z = self.forpass(x)\n",
    "        return z>0\n",
    "\n",
    "    def score(self, x, y):\n",
    "        return np.mean(self.predict(x)==y.reshape(-1,1))\n",
    "\n",
    "    def update_val_loss(self,x_val,y_val):\n",
    "        z = self.forpass(x_val)\n",
    "        a = self.activation(z)\n",
    "        a = np.clip(a,1e-10,1-1e-10)\n",
    "        val_loss = np.sum(-(y_val*np.log(a)+(1-y_val)*np.log(1-a)))\n",
    "        self.val_losses.append((val_loss + self.reg_loss())/len(y_val))\n",
    "\n",
    "    def reg_loss(self):\n",
    "      return self.l1*np.sum(np.abs(self.w)) + self.l2/(2*np.sum(self.w**2))\n",
    "\n",
    "\n",
    "class DualLayer(SingleLayer):\n",
    "\n",
    "  def __init__(self, units=10, learning_rate=0.1, l1=0, l2=0):\n",
    "      self.units = units\n",
    "      self.w1 = None\n",
    "      self.w2 = None\n",
    "      self.b1 = None\n",
    "      self.b2 = None\n",
    "      self.a1 = None\n",
    "      self.losses=[]\n",
    "      self.val_losses=[]\n",
    "      self.lr=learning_rate\n",
    "      self.l1=l1\n",
    "      self.l2=l2\n",
    "\n",
    "  def forpass(self,x):\n",
    "      z1 = np.dot(x,self.w1)+self.b1\n",
    "      self.a1 = self.activation(z1)\n",
    "      z2 = np.dot(self.a1,self.w2) + self.b2\n",
    "      return z2\n",
    "\n",
    "  def backpropa(self, x, err):\n",
    "      m = len(x)\n",
    "      w2_grad = np.dot(self.a1.T, err)/m\n",
    "      b2_grad = np.sum(err)/m\n",
    "      err_to_hidden = np.dot(err, self.w2.T) *self.a1 *(1-self.a1)\n",
    "      w1_grad = np.dot(x.T, err_to_hidden)/m\n",
    "      b1_grad = np.sum(err_to_hidden)/m\n",
    "      return w1_grad, b1_grad, w2_grad, b2_grad\n",
    "\n",
    "  def init_weights(self, n_features):\n",
    "      self.w1 = np.ones((n_features, self.units))\n",
    "      self.b1 = np.zeros(self.units)\n",
    "      self.w2 = np.ones((self.units, 1))\n",
    "      self.b2 = 0\n",
    "\n",
    "  def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "      y = y.reshape(-1,1)\n",
    "      y_val = y_val.reshape(-1,1)\n",
    "      m = len(x)\n",
    "      self.init_weights(x.shape[1])\n",
    "      for i in range(epochs):\n",
    "          a = self.training(x, y, m)\n",
    "          a = np.clip(a,1e-10,1-1e-10)\n",
    "          loss =  np.sum(-(y*np.log(a)+(1-y)*np.log(1-a)))\n",
    "          self.losses.append((loss+self.reg_loss())/m)\n",
    "          self.update_val_loss(x_val,y_val)\n",
    "\n",
    "  def training(self, x, y, m):\n",
    "      z = self.forpass(x)\n",
    "      a = self.activation(z)\n",
    "      err = -(y-a)\n",
    "      w1_grad, b1_grad, w2_grad, b2_grad = self.backpropa(x,err)\n",
    "      w1_grad += (self.l1*np.sign(np.sign(self.w1)) + self.l2*self.w1)/m\n",
    "      w2_grad += (self.l1*np.sign(np.sign(self.w2)) + self.l2*self.w2)/m\n",
    "      self.w1 -= self.lr*w1_grad\n",
    "      self.w2 -= self.lr*w2_grad\n",
    "      self.b1 -= self.lr*b1_grad\n",
    "      self.b2 -= self.lr*b2_grad\n",
    "      return a\n",
    "\n",
    "  def reg_loss(self):\n",
    "      return self.l1*(np.sum(np.abs(self.w1)) + np.sum(np.abs(self.w2))) + self.l2/2*(np.sum(np.abs(self.w1**2))+np.sum(np.abs(self.w2**2)))\n",
    "\n",
    "\n",
    "class RandomInitNetwork(DualLayer):\n",
    "\n",
    "    \n",
    "    def init_weights(self,n_features):\n",
    "        np.random.seed(42)\n",
    "        self.w1 = np.random.normal(0,1,(n_features,self.units))\n",
    "        self.b1 = np.zeros(self.units)\n",
    "        self.w2 = np.random.normal(0,1,(self.units,1))\n",
    "        self.b2 = 0\n",
    "\n",
    "\n",
    "class MiniBatchNetwork(RandomInitNetwork):\n",
    "\n",
    "    def __init__(self, units=10, batch_size=32, learning_rate=0.1, l1=0, l2=0):\n",
    "        super().__init__(units, learning_rate, l1, l2)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        self.init_weights(x.shape[1])\n",
    "        y_val = y_val.reshape(-1,1)\n",
    "        np.random.seed(42)\n",
    "        for i in range(epochs):\n",
    "            loss = 0\n",
    "            for x_batch, y_batch in self.gen_betch(x,y):\n",
    "                y_batch = y_batch.reshape(-1,1)\n",
    "                m = len(x_batch)\n",
    "                a = self.training(x_batch, y_batch, m)\n",
    "                a = np.clip(a, 1e-10, 1-1e-10)\n",
    "                loss += np.sum(-(y_batch*np.log(a)+(1-y_batch)*np.log(1-a)))\n",
    "            self.losses.append((loss+self.reg_loss())/len(x))\n",
    "            self.update_val_loss(x_val,y_val)\n",
    "\n",
    "    def gen_betch(self, x, y):\n",
    "        length = len(x)\n",
    "        bins = length // self.batch_size\n",
    "        if length%self.batch_size:\n",
    "            bins += 1\n",
    "        indexes = np.random.permutation(np.arange(len(x)))\n",
    "        x = x[indexes]\n",
    "        y = y[indexes]\n",
    "        for i in range(bins):\n",
    "            start = self.batch_size*i\n",
    "            end = self.batch_size*(i+1)\n",
    "            yield x[start:end], y[start:end]\n",
    "\n",
    "\n",
    "class MultiClassNetwork:\n",
    "\n",
    "\n",
    "    def __init__(self, units=10, batch_size=32, learning_rate=0.1, l1=0, l2=0):\n",
    "        self.units = units\n",
    "        self.batch_size = batch_size\n",
    "        self.w1 = None\n",
    "        self.w2 = None\n",
    "        self.b1 = None\n",
    "        self.b2 = None\n",
    "        self.a1 = None\n",
    "        self.losses=[]\n",
    "        self.val_losses=[]\n",
    "        self.lr=learning_rate\n",
    "        self.l1=l1\n",
    "        self.l2=l2\n",
    "\n",
    "    def forpass(self,x):\n",
    "        z1 = np.dot(x,self.w1)+self.b1\n",
    "        self.a1 = self.sigmoid(z1)\n",
    "        z2 = np.dot(self.a1,self.w2) + self.b2\n",
    "        return z2\n",
    "\n",
    "    def backpropa(self, x, err):\n",
    "        m = len(x)\n",
    "        w2_grad = np.dot(self.a1.T, err)/m\n",
    "        b2_grad = np.sum(err)/m\n",
    "        err_to_hidden = np.dot(err, self.w2.T) *self.a1 *(1-self.a1)\n",
    "        w1_grad = np.dot(x.T, err_to_hidden)/m\n",
    "        b1_grad = np.sum(err_to_hidden)/m\n",
    "        return w1_grad, b1_grad, w2_grad, b2_grad\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z,-100,None)\n",
    "        a = 1/(1+np.exp(-z))\n",
    "        return a\n",
    "\n",
    "    def softmax(self, z):\n",
    "        z = np.clip(z,None,100)\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z/np.sum(exp_z, axis=1).reshape(-1,1)\n",
    "\n",
    "    def init_weights(self, n_features, n_classes):\n",
    "        self.w1 = np.random.normal(0,1,(n_features, self.units))\n",
    "        self.b1 = np.zeros(self.units)\n",
    "        self.w2 = np.random.normal(0,1,(self.units, n_classes))\n",
    "        self.b2 = np.zeros(n_classes)\n",
    "\n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "          self.init_weights(x.shape[1],y.shape[1])\n",
    "          np.random.seed(42)\n",
    "          for i in range(epochs):\n",
    "              loss = 0\n",
    "              print('.',end='')\n",
    "              for x_batch, y_batch in self.gen_betch(x,y):\n",
    "                  a = self.training(x_batch, y_batch)\n",
    "                  a = np.clip(a, 1e-10, 1-1e-10)\n",
    "                  loss += np.sum(y_batch*np.log(a))\n",
    "              self.losses.append((loss+self.reg_loss())/len(x))\n",
    "              self.update_val_loss(x_val,y_val)\n",
    "\n",
    "    def gen_betch(self, x, y):\n",
    "        length = len(x)\n",
    "        bins = length // self.batch_size\n",
    "        if length%self.batch_size:\n",
    "            bins += 1\n",
    "        indexes = np.random.permutation(np.arange(len(x)))\n",
    "        x = x[indexes]\n",
    "        y = y[indexes]\n",
    "        for i in range(bins):\n",
    "            start = self.batch_size*i\n",
    "            end = self.batch_size*(i+1)\n",
    "            yield x[start:end], y[start:end]\n",
    "\n",
    "    def training(self, x, y):\n",
    "        m = len(x)\n",
    "        z = self.forpass(x)\n",
    "        a = self.softmax(z)\n",
    "        err = -(y-a)\n",
    "        w1_grad, b1_grad, w2_grad, b2_grad = self.backpropa(x,err)\n",
    "        w1_grad += (self.l1*np.sign(np.sign(self.w1)) + self.l2*self.w1)/m\n",
    "        w2_grad += (self.l1*np.sign(np.sign(self.w2)) + self.l2*self.w2)/m\n",
    "        self.w1 -= self.lr*w1_grad\n",
    "        self.w2 -= self.lr*w2_grad\n",
    "        self.b1 -= self.lr*b1_grad\n",
    "        self.b2 -= self.lr*b2_grad\n",
    "        return a\n",
    "\n",
    "    def predict(self, x):\n",
    "        z = self.forpass(x)\n",
    "        return np.argmax(z,axis=1)\n",
    "        \n",
    "    def score(self, x, y):\n",
    "        return np.mean(self.predict(x)==np.argmax(y, axis=1))\n",
    "\n",
    "    def reg_loss(self):\n",
    "        return self.l1*(np.sum(np.abs(self.w1)) + np.sum(np.abs(self.w2))) + self.l2/2*(np.sum(np.abs(self.w1**2))+np.sum(np.abs(self.w2**2)))\n",
    "\n",
    "    def update_val_loss(self, x_val, y_val):\n",
    "        z = self.forpass(x_val)\n",
    "        a = self.softmax(z)\n",
    "        a = np.clip(a,1e-10,1-1e-10)\n",
    "        val_loss = np.sum(-y_val*np.log(a))\n",
    "        self.val_losses.append((val_loss+self.reg_loss())/len(y_val))\n",
    "\n",
    "\n",
    "class ConvolutionNetwork:\n",
    "\n",
    "    def __init__(self, n_kernels=10, units=10, batch_size=32, learning_rate=0.1):\n",
    "        self.n_kernels = n_kernels\n",
    "        self.kernel_size = 3\n",
    "        self.optimizer = None\n",
    "        self.conv_w = None\n",
    "        self.conv_b = None\n",
    "        self.w1 = None\n",
    "        self.b1 = None\n",
    "        self.w2 = None\n",
    "        self.b2 = None\n",
    "        self.units = units\n",
    "        self.batch_size = batch_size\n",
    "        self.a1 = None\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.lr = learning_rate\n",
    "\n",
    "\n",
    "    def forpass(self, x):\n",
    "      c_out = tf.nn.conv2d(x, self.conv_w, strides=1, padding='SAME') + self.conv_b\n",
    "      r_out = tf.nn.relu(c_out)\n",
    "      p_out = tf.nn.max_pool2d(r_out, ksize=2, strides=2, padding='VALID')\n",
    "      f_out = tf.reshape(p_out, [x.shape[0],-1])\n",
    "      z1 = tf.matmul(f_out, self.w1) + self.b1\n",
    "      a1 = tf.nn.relu(z1)\n",
    "      z2 = tf.matmul(a1, self.w2) + self.b2\n",
    "      return z2\n",
    "\n",
    "\n",
    "    def init_weights(self, input_shape, n_classes):\n",
    "        g = tf.initializers.glorot_uniform()\n",
    "        self.conv_w = tf.Variable(g((3,3,1,self.n_kernels)))\n",
    "        self.conv_b = tf.Variable(np.zeros(self.n_kernels), dtype=float)\n",
    "        n_features = 14*14*self.n_kernels\n",
    "        self.w1 = tf.Variable(g((n_features, self.units)))\n",
    "        self.b1 = tf.Variable(np.zeros(self.units), dtype=float)\n",
    "        self.w2 = tf.Variable(g((self.units, n_classes)))\n",
    "        self.b2 = tf.Variable(np.zeros(n_classes), dtype=float)\n",
    "\n",
    "\n",
    "    def fit(self, x, y, x_val=None, y_val=None, epochs=100):\n",
    "        self.init_weights(x.shape, y.shape[1])\n",
    "        self.optimizer = tf.optimizers.SGD(learning_rate=self.lr)\n",
    "        for i in range(epochs):\n",
    "            print('에포크',i,end=' ')\n",
    "            batch_losses = []\n",
    "            for x_batch, y_batch in self.gen_batch(x,y):\n",
    "                print('.', end='')\n",
    "                self.training(x_batch, y_batch)\n",
    "                batch_losses.append(self.get_loss(x_batch, y_batch))\n",
    "            self.losses.append(np.mean(batch_losses))\n",
    "            self.val_losses.append(self.get_loss(x_val,y_val))\n",
    "\n",
    "      \n",
    "    def gen_batch(self, x, y):\n",
    "        bins = len(x)//self.batch_size\n",
    "        indexes = np.random.permutation(np.arange(len(x)))\n",
    "        x = x[indexes]\n",
    "        y = y[indexes]\n",
    "        for i in range(bins):\n",
    "            start = self.batch_size*i\n",
    "            end = self.batch_size*(i+1)\n",
    "            yield x[start:end], y[start:end]\n",
    "\n",
    "\n",
    "    def training(self, x, y):\n",
    "        m = len(x)\n",
    "        with tf.GradientTape() as tape:\n",
    "            z = self.forpass(x)\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(y,z)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        weights_list = [self.conv_w, self.conv_b, self.w1, self.b1, self.w2, self.b2]\n",
    "        print(loss)\n",
    "        grads = tape.gradient(loss, weights_list)\n",
    "        self.optimizer.apply_gradients(zip(grads,weights_list))\n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "        z = self.forpass(x)\n",
    "        return np.argmax(z.numpy(), axis=1)\n",
    "\n",
    "\n",
    "    def score(self, x, y):\n",
    "        return np.mean(self.predict(x)==np.argmax(y,axis=1))\n",
    "\n",
    "\n",
    "    def get_loss(self, x, y):\n",
    "        z = self.forpass(x)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y,z))\n",
    "        return loss.numpy()\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, SimpleRNN\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(x_train_all, y_train_all), (x_test,y_test) = imdb.load_data(skip_top=20, num_words=100)\n",
    "\n",
    "for i in range(len(x_train_all)):\n",
    "    x_train_all[i] = [w for w in x_train_all[i] if w>2 ]\n",
    "\n",
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {word_to_index[k]: k for k in word_to_index}\n",
    "\n",
    "np.random.seed(42)\n",
    "random_index = np.random.permutation(25000)\n",
    "x_train = x_train_all[random_index[:20000]]\n",
    "y_train = y_train_all[random_index[:20000]]\n",
    "x_val = x_train_all[random_index[20000:]]\n",
    "y_val = y_train_all[random_index[20000:]]\n",
    "maxlen = 100\n",
    "x_train_seq = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val_seq = pad_sequences(x_val, maxlen=maxlen)\n",
    "x_train_onehot = to_categorical(x_train_seq)\n",
    "x_val_onehot = to_categorical(x_val_seq)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(32, input_shape=(100,100)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ed091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
